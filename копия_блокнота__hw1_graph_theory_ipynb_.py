# -*- coding: utf-8 -*-
"""Копия блокнота "HW1.Graph theory.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XPmo2Ec3GvLrdF3DFxK9L2TSTkt6Z3aX
"""

import nltk
nltk.download('brown')
from nltk.corpus import brown
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
from typing import List
import re
import networkx as nx
from typing import List
import matplotlib.pyplot as plt
import matplotlib.font_manager
from networkx import density 
import numpy as np
from networkx.algorithms.community.centrality import girvan_newman
import itertools
from itertools import chain, combinations
from scipy.cluster.hierarchy import dendrogram

def processing(sentences:List[List[str]]):
    lemmatizer = WordNetLemmatizer()
    lemmatized = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in sentences]
    lower = [[word.lower() for word in sentence] for sentence in lemmatized]
    cleaned = [[re.sub(r'[^A-Za-z0-9]+', '', word) for word in sentence] for sentence in lower]
    stop = stopwords.words('english')
    stop.append('af')
    removing_stops = [[word for word in sentence if word not in stop] for sentence in cleaned]
    result = [[word for word in sentence if word] for sentence in removing_stops]
    
    for t in result:
      e1 = t[0][0]
      e2 = t[0][1]
      e3 = t[1]
      tt = {e1, e2, e3}
    new_list_of_tuples.append(tt)
    return(result) 
print(processing(brown.sents()[:20]))

def first_graph(input):
  pairs = []  
  for sentence in input:
      for i in range(len(sentence)-1):
       pairs.append([sentence[i], sentence[i + 1]])    
      for i in range(len(sentence)-1):
       pairs.append([sentence[i], sentence[i -1]])
  G = nx.Graph(pairs)
  G2 = nx.DiGraph(pairs)
  pos = nx.kamada_kawai_layout(G)
  #pos = nx.kamada_kawai_layout(G2)
  plt.figure(figsize=(12, 12))
  
  nx.draw(G, pos, with_labels=True,font_size=10, node_color=range(G.number_of_nodes()),node_size= range(G.number_of_nodes()), edge_color= 'grey', cmap=plt.cm.Reds)
  # or nx.draw(g2, pos, with_labels=True,font_size=10, node_color=range(g2.number_of_nodes()),node_size= range(g2.number_of_nodes()), edge_color= 'grey', cmap=plt.cm.Reds)
  return (G, pairs)
  # or return (g2,pairs)
G, ps = first_graph(processing(brown.sents()[:16]))
print(G)

"""## Descriptive analysis
Посмотрите сколько компонент, вершин, ребер. Если компонент несколько, не забывайте при анализе это учитывать. Плотность. Средняя степень вершины в графе. (2 points)
"""

print('Количество вершин: {}'.format(G.number_of_nodes()))
print('Количество рёбер: {}'.format(G. number_of_edges()))
print('Средняя степень вершины: {}'.format(round(G.number_of_edges() / float(G.number_of_nodes()), 2)))
print('Degree_centrality: {}'.format(nx.degree_centrality(G)))
print('Betweenness_centrality: {}'.format(nx.betweenness_centrality(G)))
print('Closeness_centrality: {}'.format(nx.closeness_centrality(G)))
print('Eigenvector_centrality: {}'.format(nx.eigenvector_centrality(G)))

degree = dict(G.degree())
degree_values = sorted(set(degree.values()))
hist = [list(degree.values()).count(x) for x in degree_values]
plt.figure(figsize=(10, 10))
plt.plot(degree_values, hist, 'ro-')
plt.legend(['Degree'])
plt.xlabel('Degree')
plt.ylabel('Number of nodes')
plt.show()

degree_hist = nx.degree_histogram(G) 
degree_hist = np.array(degree_hist, dtype=float)
degree_prob = degree_hist/G.number_of_nodes()
plt.loglog(np.arange(degree_prob.shape[0]),degree_prob,'b.')
plt.xlabel('k')
plt.ylabel('p(k)')
plt.title('Degree Distribution')
plt.show()

"""# Поиск сообществ"""

comp = girvan_newman(G)
res = {i:words for i, words in enumerate(tuple(sorted(c) for c in next(comp)))}
# бинарное разбиение исходного графа на сообщетва
res

subset_color = [
    "grey",
    "limegreen",
    "darkorange",
]
color = [ ]
for v in G.nodes():
  for i, words in res.items():
    if v in words:
      color.append(subset_color[i])
plt.figure(figsize=(20, 20))

nx.draw(G, node_color=color, with_labels=True)
plt.show()

k = 2
for communities in itertools.islice(comp, k):
    print({indx: words for indx, words in enumerate(tuple(sorted(c) for c in communities))})

"""## Что-то у меня ничего не получилось, сначала делала без стопслов, но там получаются очевидные связи, думала, что со стоп-словами будет интереснее, но нет:(
поэтому даже не понимаю, как проанализировать
"""